# -*- coding: utf-8 -*-
"""our_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qMr0egc8daqnljfWbrT4RuOQ_XWlQQIu
"""

import numpy as np
import torch
import utils as ut
from torch import autograd, nn, optim
from torch.nn import functional as F

reconstruction_function = nn.MSELoss()
reconstruction_function.size_average = False
nllloss = nn.NLLLoss()

class CONV(nn.Module):
    def __init__(self, in_ch, out_ch, kernel, padding, stride, flat_dim, latent_dim):
        super(CONV, self).__init__()
        self.in_ch = in_ch
        self.out_ch = out_ch
        self.kernel = kernel
        self.padding = padding
        self.stride = stride
        self.flat_dim = flat_dim
        self.latent_dim = latent_dim
        self.net = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, kernel_size=kernel, padding=padding, stride=stride),  # (w-k+2p)/s+1
            nn.BatchNorm2d(out_ch, affine=False),
            nn.PReLU()
        )
        
    def encode(self, x):
        h = self.net(x)
        # mu, var = ut.gaussian_parameters(h, dim=1)
        return h

class F1CONV(nn.Module):
    def __init__(self, in_ch, out_ch, kernel, padding, stride, flat_dim, latent_dim):
        super(F1CONV, self).__init__()
        self.in_ch = in_ch
        self.out_ch = out_ch
        self.kernel = kernel
        self.padding = padding
        self.stride = stride
        self.flat_dim = flat_dim
        self.latent_dim = latent_dim
        self.net = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, kernel_size=kernel, padding=padding, stride=stride),  # (w-k+2p)/s+1
            nn.BatchNorm2d(out_ch, affine=False),
            nn.PReLU()
        )
        self.mean_layer = nn.Sequential(
            nn.Linear(out_ch*flat_dim*flat_dim, latent_dim)
        )
        self.y_layer = nn.Sequential(
            nn.Linear(out_ch*flat_dim*flat_dim, latent_dim)
        )
        self.var_layer = nn.Sequential(
            nn.Linear(out_ch*flat_dim*flat_dim, latent_dim)
        )
        self.y_mu_layer = nn.Sequential(nn.Linear(32, latent_dim))

    def encode(self, x):
        h = self.net(x)
        #print(h.shape)
        h_flat = h.view(-1, self.out_ch*self.flat_dim*self.flat_dim)
        #print('the shape of h_flat is :', h_flat.shape)
        mu, y = self.mean_layer(h_flat), self.y_layer(h_flat)
        var = self.var_layer(h_flat)
        #print('the shape of z,y is :', mu.shape, var.shape)
        y_mu = self.y_mu_layer(y)
        z = y_mu + mu
        var = F.softplus(var) + 1e-8
        return y, z, var

class TCONV(nn.Module):
    def __init__(self, in_size, unflat_dim, t_in_ch, t_out_ch, t_kernel, t_padding, t_stride, out_dim, t_latent_dim):
        super(TCONV, self).__init__()
        self.in_size = in_size
        self.unflat_dim = unflat_dim
        self.t_in_ch = t_in_ch
        self.t_out_ch = t_out_ch
        self.t_kernel = t_kernel
        self.t_stride = t_stride
        self.t_padding = t_padding
        self.out_dim = out_dim
        self.t_latent_dim = t_latent_dim

        self.fc = nn.Linear(in_size, t_in_ch * unflat_dim * unflat_dim)
        self.net = nn.Sequential(
            nn.PReLU(),
            nn.ConvTranspose2d(t_in_ch, t_out_ch, kernel_size=t_kernel, padding=t_padding, stride=t_stride),  # (w-k+2p)/s+1
            nn.BatchNorm2d(t_out_ch, affine=False),
        )

    def decode(self, x):
        #print('the shape of x is : ',x.shape)
        x = self.fc(x)
        x = x.view(-1, self.t_in_ch, self.unflat_dim, self.unflat_dim)
        h = self.net(x)
        return h

class FCONV(nn.Module):
    def __init__(self, in_size, unflat_dim, t_in_ch, t_out_ch, t_kernel, t_padding, t_stride):
        super(FCONV, self).__init__()
        self.in_size = in_size
        self.unflat_dim = unflat_dim
        self.t_in_ch = t_in_ch
        self.t_out_ch = t_out_ch
        self.t_kernel = t_kernel
        self.t_stride = t_stride
        self.t_padding = t_padding

        self.fc_final = nn.Linear(in_size, t_in_ch * unflat_dim * unflat_dim)
        self.final = nn.Sequential(
            nn.PReLU(),
            nn.ConvTranspose2d(t_in_ch, t_out_ch, kernel_size=t_kernel, padding=t_padding, stride=t_stride),  # (w-k+2p)/s+1
            #nn.Sigmoid()
            nn.Tanh()
        )

    def decode(self,x):
        #print('the dimension of x1 is : ',x.shape)
        #x = self.fc_final(x)
        x = x.view(-1, self.t_in_ch, self.unflat_dim, self.unflat_dim)
        #print('the dimension of x is : ',x.shape)
        x_re = self.final(x)
        return x_re

class LVAE(nn.Module):
    def __init__(self, in_ch=3,
                 out_ch64=64, out_ch128=128, out_ch256=256, out_ch512=512,
                 kernel1=1, kernel2=2, kernel3=3, padding0=0, padding1=1, padding2=2, stride1=1, stride2=2,
                 flat_dim32=32, flat_dim16=16, flat_dim8=8, flat_dim4=4, flat_dim2=2, flat_dim1=1,
                 latent_dim512=512, latent_dim256=256, latent_dim128=128, latent_dim64=64, latent_dim32=32, num_class =15):
        super().__init__()
        self.in_ch = in_ch
        self.out_ch64 = out_ch64
        self.out_ch128 = out_ch128
        self.out_ch256 = out_ch256
        self.out_ch512 = out_ch512
        self.kernel1 = kernel1
        self.kernel2 = kernel2
        self.kernel3 = kernel3
        self.padding0 = padding0
        self.padding1 = padding1
        self.padding2 = padding2
        self.stride1 = stride1
        self.stride2 = stride2
        self.flat_dim32 = flat_dim32
        self.flat_dim16 = flat_dim16
        self.flat_dim8 = flat_dim8
        self.flat_dim4 = flat_dim4
        self.flat_dim2 = flat_dim2
        self.flat_dim1 = flat_dim1
        self.latent_dim512 = latent_dim512
        self.latent_dim256 = latent_dim256
        self.latent_dim128 = latent_dim128
        self.latent_dim64 = latent_dim64
        self.latent_dim32 = latent_dim32
        self.num_class = num_class

        # initialize required CONVs
        self.CONV1_1 = CONV(self.in_ch, self.out_ch64, self.kernel1, self.padding2, self.stride1, self.flat_dim32,
                            self.latent_dim512)
        self.CONV1_2 = CONV(self.out_ch64, self.out_ch64, self.kernel3, self.padding1, self.stride2, self.flat_dim16,
                            self.latent_dim512)

        self.CONV2_1 = CONV(self.out_ch64, self.out_ch128, self.kernel3, self.padding1, self.stride1, self.flat_dim16, self.latent_dim256)
        self.CONV2_2 = CONV(self.out_ch128, self.out_ch128, self.kernel3, self.padding1, self.stride2, self.flat_dim8, self.latent_dim256)

        self.CONV3_1 = CONV(self.out_ch128, self.out_ch256, self.kernel3, self.padding1, self.stride1, self.flat_dim8,
                            self.latent_dim128)
        self.CONV3_2 = CONV(self.out_ch256, self.out_ch256, self.kernel3, self.padding1, self.stride2, self.flat_dim4,
                            self.latent_dim128)

        self.CONV4_1 = CONV(self.out_ch256, self.out_ch512, self.kernel3, self.padding1, self.stride1, self.flat_dim4,
                            self.latent_dim64)
        self.CONV4_2 = CONV(self.out_ch512, self.out_ch512, self.kernel3, self.padding1, self.stride2, self.flat_dim2,
                            self.latent_dim64)

        self.CONV5_1 = CONV(self.out_ch512, self.out_ch512, self.kernel3, self.padding1, self.stride1, self.flat_dim2,
                            self.latent_dim32)
        self.CONV5_2 = F1CONV(self.out_ch512, self.out_ch512, self.kernel3, self.padding1, self.stride2, self.flat_dim1,
                            self.latent_dim32)

        # initialize required TCONVs
        self.TCONV5_2 = TCONV(self.latent_dim32, self.flat_dim1, self.out_ch512, self.out_ch512, self.kernel2,
                              self.padding0, self.stride2, self.flat_dim2, self.latent_dim32)
        self.TCONV5_1 = FCONV(self.latent_dim32, self.flat_dim2, self.out_ch512, self.out_ch512, self.kernel1,
                              self.padding0, self.stride1)

        self.TCONV4_2 = FCONV(self.latent_dim64, self.flat_dim2, self.out_ch512, self.out_ch512, self.kernel2,
                              self.padding0, self.stride2)
        self.TCONV4_1 = FCONV(self.latent_dim64, self.flat_dim4, self.out_ch512, self.out_ch256, self.kernel1,
                              self.padding0, self.stride1)

        self.TCONV3_2 = FCONV(self.latent_dim128, self.flat_dim4, self.out_ch256, self.out_ch256, self.kernel2,
                              self.padding0, self.stride2)
        self.TCONV3_1 = FCONV(self.latent_dim128, self.flat_dim8, self.out_ch256, self.out_ch128, self.kernel1,
                              self.padding0, self.stride1)

        self.TCONV2_2 = FCONV(self.latent_dim256, self.flat_dim8, self.out_ch128, self.out_ch128, self.kernel2,
                              self.padding0, self.stride2)
        self.TCONV2_1 = FCONV(self.latent_dim256, self.flat_dim16, self.out_ch128, self.out_ch64, self.kernel1,
                              self.padding0, self.stride1)

        self.TCONV1_2 = FCONV(self.latent_dim512, self.flat_dim16, self.out_ch64, self.out_ch64, self.kernel2,
                              self.padding0, self.stride2)
        self.TCONV1_1 = FCONV(self.latent_dim512, self.flat_dim32, self.out_ch64, self.in_ch, self.kernel1,
                              self.padding2, self.stride1)

        self.classifier = nn.Linear(self.latent_dim32, self.num_class)
        self.one_hot = nn.Linear(self.num_class, self.latent_dim32)

    def lnet(self, x, y_de):
        # ---deterministic upward pass
        # upwards
        enc1_1 = self.CONV1_1.encode(x)
        enc1_2 = self.CONV1_2.encode(enc1_1)

        enc2_1 = self.CONV2_1.encode(enc1_2)
        enc2_2 = self.CONV2_2.encode(enc2_1)

        enc3_1 = self.CONV3_1.encode(enc2_2)
        enc3_2 = self.CONV3_2.encode(enc3_1)

        enc4_1 = self.CONV4_1.encode(enc3_2)
        enc4_2 = self.CONV4_2.encode(enc4_1)

        enc5_1 = self.CONV5_1.encode(enc4_2)
        y, z, var = self.CONV5_2.encode(enc5_1)
        #print('the shape os z is : ',z.shape)

        latent = ut.sample_gaussian(z, var)
        predict = F.log_softmax(self.classifier(y), dim=1)
        predict_test = F.log_softmax(self.classifier(z), dim=1)
        yh = self.one_hot(y_de)

        # partially downwards
        dec5_1 = self.TCONV5_2.decode(latent)

        dec4_2 = self.TCONV5_1.decode(dec5_1)

        dec4_1 = self.TCONV4_2.decode(dec4_2)

        dec3_2 = self.TCONV4_1.decode(dec4_1)

        dec3_1 = self.TCONV3_2.decode(dec3_2)

        dec2_2 = self.TCONV3_1.decode(dec3_1)

        dec2_1 = self.TCONV2_2.decode(dec2_2)

        dec1_2 = self.TCONV2_1.decode(dec2_1)

        dec1_1 = self.TCONV1_2.decode(dec1_2)

        x_re = self.TCONV1_1.decode(dec1_1)

        return latent, z, var, predict, predict_test, yh, x_re

    def loss(self, x, y, y_de, beta, lamda):

        latent, z, var, predict, predict_test, yh, x_re = self.lnet(x, y_de)

        rec = reconstruction_function(x_re, x)

        pm, pv = torch.zeros(z.shape), torch.ones(var.shape)
        # print("mu1", mu1)
        kl_latent = ut.kl_normal(z, var, pm, pv, yh)
        kl = beta * torch.mean(kl_latent)

        ce = nllloss(predict, y)

        nelbo = rec + kl + lamda*ce
        # nelbo = rec
        return nelbo, z, predict, predict_test, x_re,rec,kl,lamda*ce

    def test(self, x, y_de):
        latent, z, var, predict, predict_test, yh, x_re = self.lnet(x, y_de)
        return z, predict_test, x_re

